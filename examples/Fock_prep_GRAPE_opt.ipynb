{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Need tf version 2.3.0 or later. Using tensorflow version: 2.8.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# import os\n",
    "# os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\" # this seems to be highly important for totally utilizing your GPU's memory, but it also breaks the profiler's memory breakdown\n",
    "# note that GradientTape needs several times the memory needed to compute the fidelity of a single circuit\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import qutip as qt\n",
    "from QOGS.optimizer.tf_adam_optimizer import AdamOptimizer\n",
    "from QOGS.gate_sets import GRAPE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_dim = 4\n",
    "c_dim = 10\n",
    "DAC_time_resolution = 2 # in ns\n",
    "fock = 1\n",
    "\n",
    "# In GHz = cycles / ns\n",
    "anharm = -.180\n",
    "kerr = -1e-5\n",
    "chi = -4e-3\n",
    "drive = D = 2 * np.pi * 1e-3\n",
    "\n",
    "a = qt.tensor(qt.destroy(c_dim), qt.qeye(q_dim))\n",
    "b = qt.tensor(qt.qeye(c_dim), qt.destroy(q_dim))\n",
    "gf = qt.tensor(qt.qeye(c_dim), qt.basis(q_dim, 0) * qt.basis(q_dim, 2).dag())\n",
    "fg = gf.dag()\n",
    "ad = a.dag()\n",
    "bd = b.dag()\n",
    "H0 = (anharm/2) * bd * bd * b * b\n",
    "H0 += (kerr/2) * ad * ad * a * a\n",
    "H0 += (chi) * ad * a * bd * b\n",
    "H0 *= 2*np.pi\n",
    "Hcs = [D*(gf + fg), 1j*D*(gf - fg), D*(a + ad), 1j*D*(a - ad)]\n",
    "\n",
    "init_states = [\n",
    "    qt.tensor(qt.basis(c_dim, 0), qt.basis(q_dim, 0))\n",
    "]\n",
    "\n",
    "final_states = [\n",
    "    qt.tensor(qt.basis(c_dim, fock), qt.basis(q_dim, 0))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_dim = 4\n",
    "c_dim = 10\n",
    "DAC_time_resolution = 1\n",
    "fock = 1\n",
    "\n",
    "# In GHz = cycles / ns\n",
    "anharm = -.180\n",
    "kerr = -1e-5\n",
    "chi = -4e-3\n",
    "drive = D = 2 * np.pi * 1e-3\n",
    "\n",
    "a = qt.tensor(qt.destroy(c_dim), qt.qeye(q_dim))\n",
    "b = qt.tensor(qt.qeye(c_dim), qt.destroy(q_dim))\n",
    "# gf = qt.tensor(qt.qeye(c_dim), qt.basis(q_dim, 0) * qt.basis(q_dim, 2).dag())\n",
    "# fg = gf.dag()\n",
    "ad = a.dag()\n",
    "bd = b.dag()\n",
    "H0 = (anharm/2) * bd * bd * b * b\n",
    "H0 += (kerr/2) * ad * ad * a * a\n",
    "H0 += (chi) * ad * a * bd * b\n",
    "H0 *= 2*np.pi\n",
    "Hcs = [D*(b + bd), 1j*D*(b - bd), D*(a + ad), 1j*D*(a - ad)] # I, Q, I, Q\n",
    "\n",
    "init_states = [\n",
    "    qt.tensor(qt.basis(c_dim, 0), qt.basis(q_dim, 0))\n",
    "]\n",
    "\n",
    "final_states = [\n",
    "    qt.tensor(qt.basis(c_dim, fock), qt.basis(q_dim, 0))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 15:29:05.148651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 15:29:05.176266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 15:29:05.176460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 15:29:05.176851: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-09 15:29:05.177351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 15:29:05.177491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 15:29:05.177614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 15:29:05.507864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 15:29:05.508039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 15:29:05.508172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-09 15:29:05.508267: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:214] Using CUDA malloc Async allocator for GPU: 0\n",
      "2022-06-09 15:29:05.508354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6528 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0a:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "synth_params = {\n",
    "    'N_blocks': 251, # note that the length of the pulse is this times the DAC_time_resolution\n",
    "    'N_multistart' : 15, #Batch size (number of circuit optimizations to run in parallel)\n",
    "    'epochs' : 300, #number of epochs before termination\n",
    "    'epoch_size' : 5, #number of adam steps per epoch\n",
    "    'learning_rate' : 0.05, #adam learning rate\n",
    "    'term_fid' : 0.995, #0.995, #terminal fidelitiy\n",
    "    'dfid_stop' : 1e-6, #stop if dfid between two epochs is smaller than this number\n",
    "    'initial_states' : init_states, #qubit tensor oscillator, start in |g> |0>\n",
    "    'target_states' : final_states, #end in |e> |target>.\n",
    "    'name' : 'GRAPE Fock %d' % fock, #name for printing and saving\n",
    "    'use_phase' : True,\n",
    "    'filename' : None, #if no filename specified, results will be saved in this folder under 'name.h5'\n",
    "}\n",
    "\n",
    "# We initialize the gateset here\n",
    "gate_set_params = {\n",
    "    'H_static' : H0,\n",
    "    'H_control' : Hcs,\n",
    "    'DAC_delta_t' : DAC_time_resolution,\n",
    "    'inplace' : False, # true uses less memory, but is slower. Just use false\n",
    "    'scale' : 1.0, # range of DAC amplitudes for initial random waves\n",
    "    'bandwidth' : 0.1, #0.05\n",
    "    'gatesynthargs': synth_params\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "GRAPE_gate_set = GRAPE(**gate_set_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 15:29:06.831909: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fcd3c0084c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-09 15:29:06.831946: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA GeForce RTX 3080, Compute Capability 8.6\n",
      "2022-06-09 15:29:06.843823: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-09 15:29:07.926763: I tensorflow/compiler/jit/xla_compilation_cache.cc:402] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-09 15:29:08.052180: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x7088610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.473418e-05"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create optimization object. \n",
    "#initial params will be randomized upon creation\n",
    "opt = AdamOptimizer(GRAPE_gate_set)\n",
    "\n",
    "#print optimization info. this lives in gatesynth, since we eventually want to fully abstract away the optimizer\n",
    "GRAPE_gate_set.best_fidelity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2022-06-09 15:29:08\n",
      " Epoch: 293 / 300 Max Fid: 0.995073 Avg Fid: 0.994554 Max dFid: 0.000198 Avg dFid: 0.000184 Elapsed time: 0:06:59.553071 Expected remaining time: 0:00:10.023452\n",
      "\n",
      " Optimization stopped. Term fidelity reached.\n",
      "\n",
      "name: GRAPE Fock 1\n",
      "scale: 1.0\n",
      "N_blocks: 251\n",
      "optimization_type: state transfer\n",
      "optimization_masks: {'I_DC0': None, 'I_real0': None, 'I_imag0': None, 'Q_DC0': None, 'Q_real0': None, 'Q_imag0': None, 'I_DC1': None, 'I_real1': None, 'I_imag1': None, 'Q_DC1': None, 'Q_real1': None, 'Q_imag1': None}\n",
      "target_unitary: None\n",
      "expectation_operators: None\n",
      "target_expectation_values: None\n",
      "N_multistart: 15\n",
      "term_fid: 0.995\n",
      "dfid_stop: 1e-06\n",
      "learning_rate: 0.05\n",
      "epoch_size: 5\n",
      "epochs: 300\n",
      "filename: None\n",
      "comment: \n",
      "use_phase: True\n",
      "timestamps: ['2022-06-09 15:29:08']\n",
      "do_prints: True\n",
      "filename: GRAPE Fock 1.h5\n",
      "\n",
      "Best circuit parameters found:\n",
      "I0:    tf.Tensor(\n",
      "[-3.29968 -3.0901  -2.75283 -2.32553 -1.84976 -1.36769 -0.91888 -0.53726\n",
      " -0.2487  -0.06928 -0.00432 -0.04834 -0.18586 -0.39297 -0.6396  -0.89216\n",
      " -1.11647 -1.28062 -1.35769 -1.32788 -1.18018 -0.91315 -0.53506 -0.0632\n",
      "  0.47757  1.0567   1.64039  2.1941   2.68512  3.08494  3.3712   3.52914\n",
      "  3.55239  3.44314  3.21162  2.87505  2.45603  1.98072  1.47676  0.97126\n",
      "  0.48894  0.05062 -0.32793 -0.63661 -0.87109 -1.03247 -1.12654 -1.16267\n",
      " -1.15254 -1.1088  -1.04383 -0.96863 -0.89198 -0.82001 -0.756   -0.70064\n",
      " -0.65246 -0.60857 -0.56548 -0.5199  -0.46952 -0.41357 -0.35318 -0.2914\n",
      " -0.23298 -0.1839  -0.15067 -0.13951 -0.15547 -0.20174 -0.27892 -0.38475\n",
      " -0.51396 -0.65855 -0.80839 -0.95201 -1.07766 -1.17451 -1.23375 -1.24965\n",
      " -1.22038 -1.14847 -1.04093 -0.90892 -0.76703 -0.63217 -0.52221 -0.4544\n",
      " -0.44376 -0.50161 -0.63429 -0.84223 -1.11951 -1.45391 -1.82748 -2.21762\n",
      " -2.59857 -2.94325 -3.22525 -3.42079 -3.51065 -3.48168 -3.32794 -3.05127\n",
      " -2.66134 -2.175   -1.61513 -1.00905 -0.3865   0.22247  0.78972  1.29103\n",
      "  1.70779  2.02825  2.24823  2.37123  2.40787  2.37485  2.2934   2.18738\n",
      "  2.08124  1.99794  1.95704  1.97307  2.05444  2.20276  2.41282  2.67309\n",
      "  2.96682  3.27342  3.57029  3.8347   4.04567  4.18567  4.24206  4.20806\n",
      "  4.08318  3.87326  3.58984  3.24923  2.8711   2.47693  2.0883   1.72527\n",
      "  1.40493  1.14025  0.93931  0.80497  0.735    0.72258  0.75722  0.82588\n",
      "  0.91426  1.00815  1.09459  1.16295  1.20568  1.21866  1.20132  1.15629\n",
      "  1.08886  1.00613  0.91608  0.8266   0.74455  0.67501  0.62079  0.58214\n",
      "  0.5568   0.54038  0.52689  0.50948  0.48132  0.4364   0.37031  0.28084\n",
      "  0.16825  0.03546 -0.11225 -0.26765 -0.42227 -0.56723 -0.6941  -0.79585\n",
      " -0.8675  -0.90676 -0.91424 -0.89351 -0.8508  -0.7944  -0.73386 -0.67911\n",
      " -0.63945 -0.6226  -0.63394 -0.67593 -0.74788 -0.84597 -0.9637  -1.09251\n",
      " -1.22278 -1.34483 -1.45008 -1.53205 -1.58721 -1.61552 -1.62061 -1.60963\n",
      " -1.59254 -1.5812  -1.5881  -1.62488 -1.70085 -1.82161 -1.98785 -2.19466\n",
      " -2.43111 -2.68057 -2.9215  -3.12874 -3.27532 -3.33458 -3.28243 -3.0996\n",
      " -2.77374 -2.30105 -1.68738 -0.9487  -0.11075  0.79204  1.71834  2.62236\n",
      "  3.45669  4.17543  4.73734  5.10878  5.26616  5.19784  4.90514  4.40252\n",
      "  3.71684  2.88568  1.95499  0.97602  0.00198 -0.91555 -1.72995 -2.40267\n",
      " -2.9056  -3.22273 -3.35091], shape=(251,), dtype=float32)\n",
      "Q0:    tf.Tensor(\n",
      "[ 1.07088  1.75126  2.29698  2.68425  2.90237  2.95405  2.85474  2.63098\n",
      "  2.31794  1.9563   1.58884  1.2569   0.99704  0.83818  0.79932  0.88826\n",
      "  1.10107  1.42265  1.82806  2.28472  2.7551   3.19976  3.58055  3.86357\n",
      "  4.02178  4.037    3.90118  3.61686  3.19676  2.66269  2.04365  1.37359\n",
      "  0.68878  0.02513 -0.58437 -1.1118  -1.53609 -1.84403 -2.03079 -2.09958\n",
      " -2.06087 -1.93097 -1.73028 -1.48142 -1.20721 -0.92889 -0.66458 -0.42822\n",
      " -0.22885 -0.07059  0.04711  0.12866  0.18129  0.2138   0.23527  0.25388\n",
      "  0.27593  0.30512  0.34229  0.38545  0.43019  0.47041  0.49925  0.5101\n",
      "  0.49758  0.45847  0.39224  0.30146  0.19168  0.07109 -0.05023 -0.1613\n",
      " -0.25131 -0.31089 -0.33309 -0.31437 -0.2551  -0.15981 -0.03697  0.10165\n",
      "  0.24195  0.36883  0.46763  0.52566  0.53357  0.48659  0.38533  0.23609\n",
      "  0.0508  -0.1537  -0.35678 -0.53555 -0.66671 -0.72846 -0.70247 -0.57556\n",
      " -0.34108  0.0002   0.43977  0.96172  1.54358  2.1578   2.77365  3.35937\n",
      "  3.88448  4.322    4.65045  4.85543  4.93061  4.8782   4.70865  4.43976\n",
      "  4.09524  3.70274  3.29167  2.89088  2.52639  2.21939  1.9847   1.82974\n",
      "  1.75411  1.74985  1.80231  1.89158  1.99427  2.08567  2.14184  2.14177\n",
      "  2.06911  1.91357  1.67181  1.34768  0.95186  0.50101  0.01636 -0.478\n",
      " -0.9571  -1.397   -1.77668 -2.07958 -2.29476 -2.41756 -2.4498  -2.39929\n",
      " -2.27906 -2.10604 -1.89956 -1.67971 -1.4657  -1.27435 -1.11896 -1.0084\n",
      " -0.9468  -0.93351 -0.96362 -1.02876 -1.11817 -1.21996 -1.32233 -1.41479\n",
      " -1.48907 -1.53981 -1.56495 -1.56562 -1.54587 -1.51192 -1.47138 -1.43219\n",
      " -1.40167 -1.3856  -1.3875  -1.40821 -1.44572 -1.49538 -1.55034 -1.60235\n",
      " -1.6426  -1.66273 -1.65577 -1.61699 -1.54449 -1.43954 -1.30656 -1.15284\n",
      " -0.98786 -0.82248 -0.66787 -0.53444 -0.43081 -0.36292 -0.33341 -0.34133\n",
      " -0.38215 -0.44816 -0.52923 -0.61377 -0.68987 -0.74651 -0.77471 -0.76839\n",
      " -0.72511 -0.64629 -0.5371  -0.40594 -0.26351 -0.12162  0.00815  0.11594\n",
      "  0.19488  0.24218  0.25988  0.25507  0.23966  0.22954  0.24337  0.30086\n",
      "  0.42085  0.61925  0.90707  1.28878  1.76098  2.31177  2.92074  3.55972\n",
      "  4.19422  4.78554  5.2933   5.67841  5.90606  5.94855  5.78783  5.41738\n",
      "  4.84339  4.08495  3.17345  2.1509   1.06756 -0.02119 -1.05842 -1.98935\n",
      " -2.76509 -3.3459  -3.70394 -3.82513 -3.7101  -3.37403 -2.84553 -2.16445\n",
      " -1.37905 -0.54241  0.29134], shape=(251,), dtype=float32)\n",
      "I1:    tf.Tensor(\n",
      "[ 5.83683  4.4333   2.85349  1.18454 -0.48125 -2.05247 -3.44455 -4.58526\n",
      " -5.41913 -5.91083 -6.04693 -5.83619 -5.30829 -4.51109 -3.50675 -2.36694\n",
      " -1.16755  0.01667  1.1172   2.07611  2.84949  3.40968  3.7461   3.86486\n",
      "  3.78704  3.54597  3.18366  2.74681  2.28271  1.83523  1.44148  1.12913\n",
      "  0.91474  0.80315  0.78784  0.85239  0.97259  1.11924  1.26119  1.36847\n",
      "  1.41511  1.38151  1.25609  1.03623  0.72823  0.34659 -0.08757 -0.54852\n",
      " -1.0084  -1.43972 -1.81781 -2.1229  -2.34177 -2.46862 -2.50537 -2.4612\n",
      " -2.35138 -2.19571 -2.01645 -1.8362  -1.67577 -1.55224 -1.47752 -1.45731\n",
      " -1.4908  -1.5709  -1.68507 -1.8167  -1.94674 -2.05566 -2.12532 -2.14069\n",
      " -2.09126 -1.97204 -1.78394 -1.53368 -1.23309 -0.89799 -0.54671 -0.19836\n",
      "  0.12885  0.4193   0.66124  0.8478   0.97747  1.05404  1.0861   1.08605\n",
      "  1.06879  1.05017  1.04539  1.06751  1.12608  1.2263   1.36842  1.54775\n",
      "  1.75512  1.9777   2.20027  2.40662  2.5811   2.71007  2.78319  2.79436\n",
      "  2.74228  2.63055  2.46733  2.2645   2.03664  1.79956  1.56896  1.35888\n",
      "  1.18056  1.04137  0.94431  0.88777  0.86584  0.86905  0.88533  0.90133\n",
      "  0.90382  0.88106  0.82402  0.72734  0.58995  0.4152   0.21064 -0.01265\n",
      " -0.24113 -0.46    -0.65465 -0.81203 -0.92192 -0.97791 -0.97806 -0.92508\n",
      " -0.82617 -0.69232 -0.53733 -0.37652 -0.22532 -0.09781 -0.00543  0.04412\n",
      "  0.04755  0.00632 -0.07354 -0.18204 -0.30619 -0.43128 -0.54231 -0.62551\n",
      " -0.66965 -0.66724 -0.61526 -0.51551 -0.37455 -0.20307 -0.01494  0.17411\n",
      "  0.34795  0.49169  0.59303  0.64359  0.63973  0.58299  0.48004  0.34212\n",
      "  0.18406  0.02291 -0.12368 -0.23906 -0.30924 -0.32427 -0.27922 -0.17476\n",
      " -0.01716  0.18216  0.4077   0.64123  0.86354  1.05622  1.20344  1.29351\n",
      "  1.3201   1.28288  1.18775  1.04629  0.87484  0.69296  0.52159  0.38108\n",
      "  0.28918  0.25918  0.29858  0.40805  0.58128  0.80522  1.06123  1.32654\n",
      "  1.57641  1.78629  1.93427  2.00316  1.98232  1.86891  1.66842  1.39444\n",
      "  1.06772  0.71441  0.36387  0.04597 -0.21168 -0.38643 -0.46292 -0.43485\n",
      " -0.30597 -0.09014  0.18948  0.50217  0.81223  1.08203  1.27553  1.36173\n",
      "  1.31795  1.13249  0.8065   0.3548  -0.19441 -0.80105 -1.41571 -1.98318\n",
      " -2.44665 -2.75227 -2.85373 -2.71646 -2.32116 -1.66625 -0.76907  0.33428\n",
      "  1.59058  2.93251  4.28276  5.55913  6.68008  7.57046  8.16687  8.42241\n",
      "  8.31024  7.82585  6.98782], shape=(251,), dtype=float32)\n",
      "Q1:    tf.Tensor(\n",
      "[-6.9046  -7.86097 -8.47206 -8.70557 -8.55218 -8.02607 -7.16377 -6.02143\n",
      " -4.6709  -3.19462 -1.68002 -0.21359  1.12474  2.26671  3.15976  3.7699\n",
      "  4.08306  4.10507  3.86015  3.38813  2.74071  1.97694  1.15852  0.34504\n",
      " -0.41033 -1.06433 -1.58625 -1.9593  -2.18087 -2.26157 -2.22328 -2.09634\n",
      " -1.91626 -1.72011 -1.54306 -1.41521 -1.3591  -1.38799 -1.50508 -1.70365\n",
      " -1.9681  -2.27581 -2.59943 -2.9097  -3.17815 -3.37977 -3.49521 -3.51241\n",
      " -3.42752 -3.245   -2.97705 -2.64224 -2.2637  -1.86692 -1.47736 -1.11823\n",
      " -0.80844 -0.5611  -0.38256 -0.27206 -0.22218 -0.21973 -0.24728 -0.285\n",
      " -0.31267 -0.31171 -0.26694 -0.16806 -0.01061  0.20368  0.46709  0.76666\n",
      "  1.08548  1.40425  1.70314  1.96356  2.16987  2.31077  2.38025  2.37804\n",
      "  2.30952  2.18513  2.01929  1.82895  1.63203  1.44564  1.2846   1.15999\n",
      "  1.07829  1.04079  1.04363  1.07829  1.13246  1.19136  1.23922  1.26084\n",
      "  1.24312  1.17633  1.05509  0.87891  0.65224  0.38414  0.08738 -0.22263\n",
      " -0.52933 -0.81644 -1.06946 -1.27691 -1.43134 -1.52991 -1.57451 -1.57152\n",
      " -1.53108 -1.46596 -1.39038 -1.3185  -1.26305 -1.23411 -1.23814 -1.27736\n",
      " -1.34957 -1.44841 -1.56403 -1.68407 -1.79492 -1.88309 -1.9366  -1.94616\n",
      " -1.90616 -1.8153  -1.67673 -1.4979  -1.28984 -1.06621 -0.84203 -0.6323\n",
      " -0.45061 -0.30786 -0.21126 -0.16366 -0.1633  -0.20399 -0.27573 -0.36567\n",
      " -0.45939 -0.54227 -0.60093 -0.62456 -0.60597 -0.54233 -0.43546 -0.2917\n",
      " -0.12133  0.06242  0.24462  0.4101   0.54489  0.63769  0.6809   0.67153\n",
      "  0.61147  0.50744  0.3704   0.21458  0.05611 -0.08843 -0.20365 -0.27683\n",
      " -0.29916 -0.2667  -0.18077 -0.04798  0.12039  0.30934  0.50155  0.67901\n",
      "  0.8247   0.92429  0.96753  0.9493   0.87021  0.73662  0.56016  0.35665\n",
      "  0.14472 -0.05601 -0.22677 -0.35152 -0.41855 -0.42176 -0.36137 -0.24412\n",
      " -0.08272  0.10511  0.2983   0.47429  0.61124  0.69012  0.69668  0.62297\n",
      "  0.46838  0.23995 -0.04798 -0.37486 -0.7156  -1.04277 -1.32905 -1.54984\n",
      " -1.68558 -1.72383 -1.66064 -1.50127 -1.26    -0.95918 -0.62738 -0.29699\n",
      " -0.00125  0.22882  0.36752  0.39718  0.31029  0.11081 -0.18553 -0.55195\n",
      " -0.95234 -1.34401 -1.68106 -1.91833 -2.01545 -1.94075 -1.67461 -1.21209\n",
      " -0.56436  0.24099  1.16106  2.13997  3.11235  4.00778  4.75584  5.29131\n",
      "  5.55931  5.5198   5.15116  4.45258  3.4449   2.17002  0.68871 -0.92299\n",
      " -2.57852 -4.18639 -5.65619], shape=(251,), dtype=float32)\n",
      "\n",
      " Best circuit Fidelity: 0.218933\n",
      "\n",
      "\n",
      "all data saved as: GRAPE Fock 1.h5\n",
      "termination reason: term_fid\n",
      "optimization timestamp (start time): 2022-06-09 15:29:08\n",
      "timestamp (end time): 2022-06-09 15:36:07\n",
      "elapsed time: 0:06:59.558694\n",
      "Time per epoch (epoch size = 5): 0:00:01.431941\n",
      "Time per Adam step (N_multistart = 15): 0:00:00.004773\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2022-06-09 15:29:08'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run optimizer.\n",
    "#note the optimizer can be stopped at any time by interrupting the python consle,\n",
    "#and the optimization results will still be saved and part of the opt object.\n",
    "#This allows you to stop the optimization whenever you want and still use the result.\n",
    "# Note that you will not want to use the performance profiler while using 'inplace' mode. You will run out of memory\n",
    "opt.optimize()#logdir='logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPE_gate_set.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard and navigate to the Profile tab to view performance profile\n",
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPE.get_IQ_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('/workspaces/QOGS/examples/GRAPE Fock 1.h5', 'r')\n",
    "pulse_obj = f[list(f.keys())[len(list(f.keys())) - 1]] # get the latest key\n",
    "# pulse_obj = f['2022-06-08 18:42:44']\n",
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fids = pulse_obj['fidelities']\n",
    "fids.shape\n",
    "pulse_idx = np.argmax(np.amax(fids, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = GRAPE_gate_set.get_IQ_time_series(pulse_obj['I_DC1'][-1, :,:].T, pulse_obj['I_real1'][-1, :,:].T,\n",
    "                            pulse_obj['I_imag1'][-1, :,:].T,\n",
    "                            pulse_obj['Q_DC1'][-1, :,:], pulse_obj['Q_real1'][-1, :,:].T,\n",
    "                            pulse_obj['Q_imag1'][-1, :,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = np.zeros((301), dtype=np.complex64)\n",
    "zeros[1] = (1-1j)\n",
    "zeros[-1] = (1+1j)\n",
    "zeros[10] = (1+1j) * 1j\n",
    "zeros[-10] = (1-1j) * 1j\n",
    "boring = tf.constant(zeros, dtype=tf.complex64)\n",
    "\n",
    "# plt.plot(test[0:50, :])\n",
    "plt.plot(tf.signal.ifft(zeros).numpy().real)\n",
    "plt.plot(tf.signal.ifft(zeros).numpy().imag)\n",
    "# plt.plot(tf.signal.fft(tf.signal.ifft(zeros)).numpy().real)\n",
    "# plt.plot(np.fft.ifft(zeros).real)\n",
    "# plt.plot(np.fft.ifft(zeros).imag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
