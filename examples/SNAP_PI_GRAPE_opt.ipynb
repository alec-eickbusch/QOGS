{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Need tf version 2.3.0 or later. Using tensorflow version: 2.8.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# import os\n",
    "# os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\" # this seems to be highly important for totally utilizing your GPU's memory, but it also breaks the profiler's memory breakdown\n",
    "# note that GradientTape needs several times the memory needed to compute the fidelity of a single circuit\n",
    "\n",
    "import numpy as np\n",
    "import qutip as qt\n",
    "from QOGS.optimizer.tf_adam_optimizer import AdamOptimizer\n",
    "from QOGS.gate_sets import PI_GRAPE, GRAPE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_dim = 3\n",
    "c_dim = 10\n",
    "DAC_time_resolution = 2 # in ns\n",
    "\n",
    "# In GHz = cycles / ns\n",
    "anharm = -.180\n",
    "kerr = -1e-5*0\n",
    "chi = -0.25e-3\n",
    "chi_prime = 0\n",
    "drive = D = 2 * np.pi * 1e-3\n",
    "\n",
    "a = qt.tensor(qt.destroy(c_dim), qt.qeye(q_dim))\n",
    "b = qt.tensor(qt.qeye(c_dim), qt.destroy(q_dim))\n",
    "gf = qt.tensor(qt.qeye(c_dim), qt.basis(q_dim, 0) * qt.basis(q_dim, 2).dag())\n",
    "fg = gf.dag()\n",
    "# hh = qt.tensor(qt.qeye(c_dim), qt.basis(q_dim, 3) * qt.basis(q_dim, 3).dag())\n",
    "ad = a.dag()\n",
    "bd = b.dag()\n",
    "# H0 = (anharm/2) * bd * bd * b * b\n",
    "# H0 = (3 * anharm) * hh\n",
    "H0 = (chi) * ad * a * bd * b\n",
    "H0 += (kerr/2) * ad * ad * a * a\n",
    "H0 += (chi_prime / 6) * ad * ad * a * a * bd * b\n",
    "H0 *= 2*np.pi\n",
    "Hcs = [D*(gf + fg), 1j*D*(gf - fg)]\n",
    "\n",
    "init_states = [\n",
    "    ((qt.tensor(qt.basis(c_dim, 0), qt.basis(q_dim, 0)) \n",
    "        + qt.tensor(qt.basis(c_dim, 4), qt.basis(q_dim, 0))) / np.sqrt(2)\n",
    "        + qt.tensor(qt.basis(c_dim, 2), qt.basis(q_dim, 0))).unit()\n",
    "]\n",
    "\n",
    "final_states = [\n",
    "    ((qt.tensor(qt.basis(c_dim, 0), qt.basis(q_dim, 2)) \n",
    "        + qt.tensor(qt.basis(c_dim, 4), qt.basis(q_dim, 2))) / np.sqrt(2)\n",
    "        - qt.tensor(qt.basis(c_dim, 2), qt.basis(q_dim, 2))).unit()\n",
    "]\n",
    "\n",
    "success_op = qt.tensor(qt.qeye(c_dim), qt.fock_dm(q_dim, 2))\n",
    "# jump_op = qt.tensor(qt.qeye(c_dim), qt.fock_dm(q_dim, 0))\n",
    "jump_op = qt.tensor(qt.qeye(c_dim), qt.fock_dm(q_dim, 0)) / 2 - qt.tensor(qt.qeye(c_dim), qt.fock_dm(q_dim, 2)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-13 03:23:09.320044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-13 03:23:09.347012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-13 03:23:09.347205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-13 03:23:09.348137: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-13 03:23:09.348753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-13 03:23:09.348975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-13 03:23:09.349164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-13 03:23:09.675700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-13 03:23:09.675875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-13 03:23:09.676007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:952] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-13 03:23:09.676100: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:214] Using CUDA malloc Async allocator for GPU: 0\n",
      "2022-06-13 03:23:09.676192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6652 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0a:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4808"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PI = True\n",
    "synth_params = {\n",
    "    'N_blocks': 601, # note that the length of the pulse is this times the DAC_time_resolution\n",
    "    'N_multistart' : 16, #Batch size (number of circuit optimizations to run in parallel)\n",
    "    'epochs' : 3000, #number of epochs before termination\n",
    "    'epoch_size' : 5, #number of adam steps per epoch\n",
    "    'learning_rate' : 0.1, #adam learning rate\n",
    "    'term_fid' : 0.99, #0.995, #terminal fidelitiy\n",
    "    'dfid_stop' : 1e-5/2, #stop if dfid between two epochs is smaller than this number\n",
    "    'initial_states' : init_states, #qubit tensor oscillator, start in |g> |0>\n",
    "    'target_states' : final_states, #end in |e> |target>.\n",
    "    'name' : 'PI GRAPE SNAP binom Z' if PI else 'GRAPE SNAP binom Z', #name for printing and saving\n",
    "    'use_phase' : True,\n",
    "    'filename' : None, #if no filename specified, results will be saved in this folder under 'name.h5'\n",
    "}\n",
    "\n",
    "# We initialize the gateset here\n",
    "gate_set_params = {\n",
    "    'H_static' : H0,\n",
    "    'H_control' : Hcs,\n",
    "    'DAC_delta_t' : DAC_time_resolution,\n",
    "    'inplace' : False, # true uses less memory, but is slower. Just use false\n",
    "    'scale' : 1.0, # range of DAC amplitudes for initial random waves\n",
    "    'bandwidth' : 0.1,\n",
    "    'ringup' : 40,\n",
    "    'jump_ops' : jump_op,\n",
    "    'jump_weights' : synth_params[\"N_blocks\"] * DAC_time_resolution / 50000 * 20, \n",
    "    'success_op' : success_op,\n",
    "    'success_weight' : 1.0*0,\n",
    "    'gatesynthargs': synth_params\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "if PI: GRAPE_gate_set = PI_GRAPE(**gate_set_params)\n",
    "else: GRAPE_gate_set = GRAPE(**gate_set_params)\n",
    "gate_set_params[\"jump_weights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-13 03:23:11.538290: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7fda302d8750 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-13 03:23:11.538325: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA GeForce RTX 3080, Compute Capability 8.6\n",
      "2022-06-13 03:23:11.547455: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-06-13 03:23:12.676417: I tensorflow/compiler/jit/xla_compilation_cache.cc:402] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2022-06-13 03:23:12.805651: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x7ccd870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00010426571"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create optimization object. \n",
    "#initial params will be randomized upon creation\n",
    "opt = AdamOptimizer(GRAPE_gate_set)\n",
    "\n",
    "#print optimization info. this lives in gatesynth, since we eventually want to fully abstract away the optimizer\n",
    "GRAPE_gate_set.best_fidelity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2022-06-13 03:23:13\n",
      " Epoch: 23 / 3000 Max Fid: 0.179491 Avg Fid: 0.173362 Max dFid: 0.015791 Avg dFid: 0.015556 Elapsed time: 0:02:28.440509 Expected remaining time: 5:20:13.364961"
     ]
    }
   ],
   "source": [
    "#run optimizer.\n",
    "#note the optimizer can be stopped at any time by interrupting the python consle,\n",
    "#and the optimization results will still be saved and part of the opt object.\n",
    "#This allows you to stop the optimization whenever you want and still use the result.\n",
    "# Note that you will not want to use the performance profiler while using 'inplace' mode. You will run out of memory\n",
    "opt.optimize()#logdir='logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPE_gate_set.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard and navigate to the Profile tab to view performance profile\n",
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
